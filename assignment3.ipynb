{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nicho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nicho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import math\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import numba\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Construct Term-Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the play title and text from each line of the input CSV\n",
    "shakespeare_csv_path = os.path.join(os.getcwd(), 'part_1_input_data', 'ShakespearePlays_text.csv')\n",
    "\n",
    "title_text_list = []\n",
    "\n",
    "with open(shakespeare_csv_path, 'r') as csv_file:\n",
    "    csv_data = csv.reader(csv_file)\n",
    "    for row in csv_data:\n",
    "        row_list = ''.join(row).split(';')\n",
    "        title = row_list[1][1:-1]\n",
    "        text = row_list[5].lower()\n",
    "        text = re.sub('\\n\\d+', '', text)\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        title_text_list.append((title, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the document names\n",
    "document_names_file_path = os.path.join(os.getcwd(), 'part_1_input_data', 'Shakespeare_play_names.txt')\n",
    "\n",
    "play_titles = []\n",
    "\n",
    "with open(document_names_file_path, 'r') as file:\n",
    "    play_titles = [title for title in file.read().split('\\n') if title != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse words from vocab file\n",
    "vocab_file_path = os.path.join(os.getcwd(), 'part_1_input_data', 'Shakespeare_vocab.txt')\n",
    "\n",
    "vocab_list = []\n",
    "\n",
    "with open(vocab_file_path, 'r') as file:\n",
    "    vocab_list = [word for word in file.read().split('\\n') if word != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create term-document matrix\n",
    "term_document_matrix = {}\n",
    "\n",
    "for word in vocab_list:\n",
    "    term_document_matrix[word] = {play_title: 0 for play_title in play_titles}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the term-document matrix\n",
    "for title_text_tuple in title_text_list:\n",
    "    play_title = title_text_tuple[0]\n",
    "    words = title_text_tuple[1].split()\n",
    "    for word in words:\n",
    "        if word in term_document_matrix:\n",
    "            term_document_matrix[word][play_title] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the term-document matrix dictionary\n",
    "term_document_matrix_df = pd.DataFrame.from_dict(term_document_matrix).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write 5 random non-zero frequency words in each play to output file\n",
    "term_doc_sample_file = os.path.join(os.getcwd(), 'part_1_output', 'term_doc_sample.txt')\n",
    "\n",
    "with open(term_doc_sample_file, 'w') as output_file:\n",
    "    for column in term_document_matrix_df:\n",
    "        non_zero_word_count = 0\n",
    "        while non_zero_word_count < 5:\n",
    "            word = term_document_matrix_df.sample().index[0]\n",
    "            if term_document_matrix_df[column][word] > 0:\n",
    "                line = column + ', ' + word + ', ' + str(term_document_matrix_df[column][word]) + '\\n'\n",
    "                output_file.write(line)\n",
    "                non_zero_word_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Compute Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotProduct(df, column_1, column_2):\n",
    "    \"\"\"Return the dot product of the two columns in the specified DataFrame.\"\"\"\n",
    "    return sum(df[column_1] * df[column_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorLength(df, column):\n",
    "    \"\"\"Return the vector length of the specified column in the specified DataFrame.\"\"\"\n",
    "    return math.sqrt(sum(df[column] ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineDistance(df, column_1, column_2):\n",
    "    \"\"\"Return the cosine distance of the two columns in the specified DataFrame.\"\"\"\n",
    "    dot_product = dotProduct(df, column_1, column_2)\n",
    "    vector_1_length = vectorLength(df, column_1)\n",
    "    vector_2_length = vectorLength(df, column_2)\n",
    "    length_product = vector_1_length * vector_2_length\n",
    "    if length_product != 0:\n",
    "        return dot_product / length_product\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePlayCosineDistances(term_document_matrix_df, play_titles):\n",
    "    \"\"\"\n",
    "    Return the cosine distance between each play vector, represented by a column in \n",
    "    the specified term document matrix DataFrame.\n",
    "    \"\"\"\n",
    "    play_cosine_distances = {}\n",
    "\n",
    "    for title_1 in play_titles:\n",
    "        play_cosine_distances[title_1] = {}\n",
    "        for title_2 in play_titles:\n",
    "            if not title_1 == title_2:\n",
    "                play_cosine_distances[title_1][title_2] = cosineDistance(term_document_matrix_df, title_1, title_2)\n",
    "                \n",
    "    return play_cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine distance between each play\n",
    "play_cosine_distances = computePlayCosineDistances(term_document_matrix_df, play_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each play, output the play with the highest cosine similarity to the doc_sim.txt output file\n",
    "cosine_distances_file_path = os.path.join(os.getcwd(), 'part_1_output', 'doc_sim.txt')\n",
    "\n",
    "with open(cosine_distances_file_path, 'w') as file:\n",
    "    for play in play_cosine_distances:\n",
    "        most_similar_play = max(play_cosine_distances[play], key=play_cosine_distances[play].get)\n",
    "        line = play + ', ' + most_similar_play + ', ' + str(play_cosine_distances[play][most_similar_play]) + '\\n'\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Measuring Word Similarity using Term-Context Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the term context matrix and set each cell to 0\n",
    "term_context_matrix_df = pd.DataFrame(0, columns=vocab_list, index=vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the term context matrix using a window size of 4\n",
    "# Note: I'm using columns as word vectors and rows as context words.\n",
    "# Note: this may take 10-15 min to run\n",
    "window_size = 4\n",
    "\n",
    "for title_text in title_text_list:\n",
    "    text_tokens = title_text[1].split()\n",
    "    for index, word in enumerate(text_tokens):\n",
    "        if word in vocab_list:\n",
    "            left_window_start = index - window_size if (index - window_size) > 0 else 0\n",
    "            right_window_end = index + window_size if (index + window_size) < len(text_tokens) else (len(text_tokens) - 1)\n",
    "            left_window = text_tokens[left_window_start:index]\n",
    "            right_window = text_tokens[index+1:right_window_end]\n",
    "            \n",
    "            for left_window_word in left_window:\n",
    "                if left_window_word in vocab_list:\n",
    "                    term_context_matrix_df.loc[left_window_word, word] += 1\n",
    "                    \n",
    "            for right_window_word in right_window:\n",
    "                if right_window_word in vocab_list:\n",
    "                    term_context_matrix_df.loc[right_window_word, word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target words list\n",
    "target_words = ['romeo', 'juliet', 'nobleman', 'caesar', 'friend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target words cosine similarities df\n",
    "target_word_cosine_similarities_df = pd.DataFrame(0, columns=target_words, index=vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity of each of the target words with all words in the vocabulary\n",
    "# Note: this may take 10-15 min to run\n",
    "for target_word in target_words:\n",
    "    for word in vocab_list:\n",
    "        if not target_word == word:\n",
    "            target_word_cosine_similarities_df.loc[word, target_word] = cosineDistance(term_context_matrix_df, target_word, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the top five most similar words to each of the target words to results file\n",
    "term_doc_sim_out = os.path.join(os.getcwd(), 'part_1_output', 'term_context_sim.txt')\n",
    "\n",
    "with open(term_doc_sim_out, 'w') as file:\n",
    "    for target_word in target_words:\n",
    "        most_similar_words = target_word_cosine_similarities_df[target_word].nlargest(5).index.tolist()\n",
    "        for similar_word in most_similar_words:\n",
    "            line = target_word + ', ' + similar_word + ', ' + str(target_word_cosine_similarities_df.loc[similar_word, target_word]) + '\\n'\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 TF-IDF in the Term-Context Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encode and transpose the term document matrix\n",
    "one_hot_term_doc_df = term_document_matrix_df.T.astype(bool).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf term context matrix\n",
    "tf_idf_term_context_df = term_context_matrix_df.T.copy(deep=True)\n",
    "tf_idf_term_context_df = tf_idf_term_context_df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def computeTfIdf(term_context_vector, idf):\n",
    "    \"\"\"Return vector of term context counts tranformed to tf-idf values.\"\"\"\n",
    "    return term_context_vector * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF in the term-context matrix\n",
    "for context_word in tf_idf_term_context_df:\n",
    "    document_freq = one_hot_term_doc_df[context_word].sum()\n",
    "    idf = 1 / document_freq if document_freq > 0 else 0  \n",
    "    tf_idf_term_context_df[context_word] = computeTfIdf(tf_idf_term_context_df[context_word].to_numpy(), idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cosine similarity of the tf-idf scores for the target word vectors\n",
    "# with the vectors of all other words in the vocabulary\n",
    "# Note: this takes 10-15 minutes to run\n",
    "tf_idf_df = tf_idf_term_context_df.T\n",
    "target_tf_idf_cosine_df = pd.DataFrame(0, columns=target_words, index=vocab_list)\n",
    "\n",
    "for target_word in target_words:\n",
    "    for word in vocab_list:\n",
    "        if not target_word == word:\n",
    "            cosine_distance = cosineDistance(tf_idf_df, target_word, word)\n",
    "            target_tf_idf_cosine_df.loc[word, target_word] = cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the five most similar tf-idf word vectors for each of the target words to \n",
    "# the results file\n",
    "tf_idf_sim_out = os.path.join(os.getcwd(), 'part_1_output', 'tf_idf_sim.txt')\n",
    "\n",
    "with open(tf_idf_sim_out, 'w') as file:\n",
    "    for target_word in target_words:\n",
    "        most_similar_words = target_tf_idf_cosine_df[target_word].nlargest(5).index.tolist()\n",
    "        for similar_word in most_similar_words:\n",
    "            line = target_word + ', ' + similar_word + ', ' + str(target_tf_idf_cosine_df.loc[similar_word, target_word]) + '\\n'\n",
    "            file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Neural Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Train a feed-forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data \n",
    "pos_train_files = glob.glob(os.path.join(os.getcwd(), 'imdb_train_data', 'pos', '*'))\n",
    "neg_train_files = glob.glob(os.path.join(os.getcwd(), 'imdb_train_data', 'neg', '*'))  \n",
    "pos_train_texts = [Path(file).read_text() for file in pos_train_files]\n",
    "neg_train_texts = [Path(file).read_text() for file in neg_train_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility variables\n",
    "cached_stopwords = stopwords.words('english')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "unk_token = 'UNK'\n",
    "unk_threshold = 5\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNewlines(text):\n",
    "    \"\"\"Remove newline characters from the specified text.\"\"\"\n",
    "    return re.sub('\\n', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeText(text):\n",
    "    \"\"\"Tokenize the specified text using nltk.\"\"\"\n",
    "    return nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(tokenized_text, stopwords_list):\n",
    "    \"\"\"Remove stopwords from the specified text tokens.\"\"\"\n",
    "    return [word for word in tokenized_text if not word in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuationTokens(tokenized_text):\n",
    "    \"\"\"Remove all tokens comprised of only punctuation characters from the specified text tokens.\"\"\"\n",
    "    return [word for word in tokenized_text if not all(c in string.punctuation for c in word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeTokens(tokenized_text, lemmatizer):\n",
    "    \"\"\"Lemmatize the specified list of text tokens.\"\"\"\n",
    "    return [lemmatizer.lemmatize(word) for word in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessText(text, stopwords_list, lemmatizer):\n",
    "    \"\"\"\n",
    "    Remove newline characters, stopwords, and tokens comprised of only punctuation, and \n",
    "    lemmatize the specified text.\n",
    "    \"\"\"\n",
    "    text = removeNewlines(text)\n",
    "    tokenized_text = tokenizeText(text)\n",
    "    tokenized_text = removeStopwords(tokenized_text, stopwords_list)\n",
    "    tokenized_text = removePunctuationTokens(tokenized_text)\n",
    "    lemmatized_tokens = lemmatizeTokens(tokenized_text, lemmatizer)\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all training text blocks\n",
    "for index,text in enumerate(pos_train_texts):\n",
    "    pos_train_texts[index] = preprocessText(text, cached_stopwords, wordnet_lemmatizer)\n",
    "    \n",
    "for index,text in enumerate(neg_train_texts):\n",
    "    neg_train_texts[index] = preprocessText(text, cached_stopwords, wordnet_lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCounts(text):\n",
    "    \"\"\"Return a dictionary of counts for each unique word in the specified text.\"\"\"\n",
    "    word_counts = {}\n",
    "    \n",
    "    for word in text.split():\n",
    "        if not word in word_counts.keys():\n",
    "            word_counts[word] = 1\n",
    "        else:\n",
    "            word_counts[word] += 1\n",
    "            \n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeWordsByCount(word_counts, threshold):\n",
    "    \"\"\"\n",
    "    Return a new dictionary that contains all words in the specified word_counts dictionary\n",
    "    with occurrence counts greater than the specified threshold value.\n",
    "    \"\"\"\n",
    "    new_word_counts = dict(word_counts)\n",
    "    words_to_delete = []\n",
    "    \n",
    "    for word in new_word_counts.keys():\n",
    "        if new_word_counts[word] < threshold:\n",
    "            words_to_delete.append(word)\n",
    "    \n",
    "    for word in words_to_delete:\n",
    "        del new_word_counts[word]\n",
    "    \n",
    "    return new_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a word counts dictionary using the training data\n",
    "train_text = ' '.join(pos_train_texts) + ' '.join(neg_train_texts)\n",
    "word_counts = wordCounts(train_text)\n",
    "word_counts = removeWordsByCount(word_counts, unk_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vocabulary list using the word counts dictionary\n",
    "train_vocabulary = list(word_counts.keys())\n",
    "train_vocabulary.append(unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceUnk(text, vocabulary, unk_token):\n",
    "    \"\"\"\n",
    "    Replace all tokens in the specified text string that are not in the specified vocabulary\n",
    "    with the specified unk_token.\n",
    "    \"\"\"\n",
    "    text_tokens = text.split()\n",
    "    \n",
    "    for index,word in enumerate(text_tokens):\n",
    "        if not word in word_counts.keys():\n",
    "            text_tokens[index] = unk_token\n",
    "    \n",
    "    return ' '.join(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countUnk(texts, unk_token):\n",
    "    \"\"\"Count the number of UNK characters in the specified text.\"\"\"\n",
    "    unk_count = 0;\n",
    "    \n",
    "    for text in texts:\n",
    "        text_tokens = text.split()\n",
    "        for word in text_tokens:\n",
    "            if word == unk_token:\n",
    "                unk_count += 1\n",
    "            \n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace words in the training set that are not in the vocabulary with 'UNK'\n",
    "for index,text in enumerate(pos_train_texts):\n",
    "    pos_train_texts[index] = replaceUnk(text, train_vocabulary, unk_token)\n",
    "    \n",
    "for index,text in enumerate(neg_train_texts):\n",
    "    neg_train_texts[index] = replaceUnk(text, train_vocabulary, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classes list and single train_text list\n",
    "classes = []\n",
    "\n",
    "for text in pos_train_texts:\n",
    "    classes.append(1)\n",
    "\n",
    "for text in neg_train_texts:\n",
    "    classes.append(0)\n",
    "    \n",
    "train_texts = pos_train_texts + neg_train_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count UNK instances and add UNK to word_counts dictionary\n",
    "word_counts[unk_token] = countUnk(train_texts, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a torch vocabulary by sorting the word count dictionary and converting'\n",
    "# it to an OrderedDict\n",
    "sorted_word_counts_tuples = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_word_counts = OrderedDict(sorted_word_counts_tuples)\n",
    "torch_vocab = vocab(ordered_word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset(Dataset):\n",
    "    \"\"\"Iterable DataSet class. Consists of data list and prediction target list.\"\"\"\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda to convert input text to integers (indices)\n",
    "text_pipeline = lambda x: torch_vocab(x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    \"\"\"\n",
    "    For the specified batch, return a list of labels, transformed text indices, and and offsets.\n",
    "    \n",
    "    Sources: \n",
    "        https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "    \"\"\"\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    \n",
    "    for (text, label) in batch:\n",
    "        label_probs = [1, 0] if label == 0 else [0, 1]\n",
    "        label_list.append(label_probs)\n",
    "        text_indices = torch.tensor(text_pipeline(text), dtype=torch.int64)\n",
    "        text_list.append(text_indices)\n",
    "        offsets.append(text_indices.size(0))\n",
    "        \n",
    "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    \n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifierSingleHiddenLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Text classifier model.\n",
    "    Consists of a word embedding layer, hidden layer, top layer, and activation function.\n",
    "    \n",
    "    Sources: \n",
    "        https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "        https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/\n",
    "        http://www.cse.chalmers.se/~richajo/nlp2019/l2/Text%20classification%20using%20a%20CBoW%20representation.html\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dims, num_hidden_nodes, num_classes):\n",
    "        super(TextClassifierSingleHiddenLayer, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dims, sparse=True)\n",
    "        self.hidden_layer = nn.Linear(embedding_dims, num_hidden_nodes)\n",
    "        self.top_layer = nn.Linear(num_hidden_nodes, num_classes)\n",
    "        self.act = nn.Sigmoid()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initranges = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.hidden_layer.weight.data.uniform_(-initrange, initrange)\n",
    "        self.hidden_layer.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        hidden = self.act(self.hidden_layer(embedded))\n",
    "        scores = self.top_layer(hidden)\n",
    "        return self.act(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, criterion, epoch):\n",
    "    \"\"\"\n",
    "    Train the specified model using the data in the specified dataloader.\n",
    "    \n",
    "    Sources:\n",
    "        https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_accuracy, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        # Clear the gradients for all optimizer parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Predict labels for the batch input\n",
    "        predicted_label = model(text, offsets)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(predicted_label, label)\n",
    "        \n",
    "        # Compute the gradient for every parameter\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform gradient clipping to avoid gradient explosion\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        \n",
    "        # Update parameters for current gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute the running total accuracy for this epoch\n",
    "        total_accuracy += (predicted_label.argmax(1) == label.argmax(1)).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        \n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            total_accuracy, total_count = 0, 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_dataloader, model, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the specified model with data from the test_dataloader.\n",
    "    \n",
    "    Sources:\n",
    "        https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_accuracy, total_count = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(test_dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_accuracy += (predicted_label.argmax(1) == label.argmax(1)).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_accuracy / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainTestSplits(X, y, train_indices, test_indices):\n",
    "    \"\"\"Create train/test splits from the data (X) and target (y) lists.\"\"\"\n",
    "    X_train = [X[i] for i in train_indices]\n",
    "    y_train = [y[i] for i in train_indices]\n",
    "    X_test = [X[i] for i in test_indices]\n",
    "    y_test = [y[i] for i in test_indices]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(X, y, model, model_out_path, epochs=4, n_splits=10):\n",
    "    \"\"\"Train the specified model using 10-fold cross validation.\"\"\"\n",
    "    batch_size = 3\n",
    "    learning_rate = 5\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=5)\n",
    "    for fold_index, (train_indices, test_indices) in enumerate(skf.split(X, y)):\n",
    "        X_train, y_train, X_test, y_test = createTrainTestSplits(X, y, train_indices, test_indices)\n",
    "            \n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3.0, gamma=0.1)\n",
    "        total_accuracy = None\n",
    "            \n",
    "        train_dataset = ImdbDataset(X_train, y_train)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "            \n",
    "        test_dataset = ImdbDataset(X_test, y_test)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "            \n",
    "        print('fold: {}'.format(fold_index))\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            train(train_dataloader, model, optimizer, criterion, epoch)\n",
    "            epoch_accuracy = evaluate(test_dataloader, model, criterion)\n",
    "            if total_accuracy is not None and total_accuracy > epoch_accuracy:\n",
    "                # Update the learning rate\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                total_accuracy = epoch_accuracy\n",
    "                torch.save(model.state_dict(), model_out_path)\n",
    "            print('\\t' + ('-' * 59))\n",
    "            print('\\t| end of epoch {:3d} | time: {:5.2f}s | test accuracy {:8.3f} '.format(epoch, time.time() - epoch_start_time, epoch_accuracy))\n",
    "            print('\\t' + ('-' * 59))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the single hidden layer text classifier model\n",
    "vocab_size = len(train_vocabulary)\n",
    "embedding_dims = 5\n",
    "num_hidden_nodes = 40\n",
    "num_classes = 2\n",
    "\n",
    "model = TextClassifierSingleHiddenLayer(vocab_size, embedding_dims, num_hidden_nodes, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model output path\n",
    "single_layer_model_out_path = os.path.join(os.getcwd(), 'part_2_output', 'best_single_hidden_layer_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  0.98s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   2 | time:  1.31s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   3 | time:  1.17s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   4 | time:  1.17s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "fold: 1\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  1.04s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   2 | time:  0.99s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   3 | time:  0.90s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   4 | time:  0.86s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "fold: 2\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  0.92s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   2 | time:  0.92s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   3 | time:  0.88s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   4 | time:  0.84s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "fold: 3\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  0.94s | test accuracy    0.504 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   2 | time:  0.83s | test accuracy    0.522 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   3 | time:  0.86s | test accuracy    0.619 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   4 | time:  0.95s | test accuracy    0.673 \n",
      "\t-----------------------------------------------------------\n",
      "fold: 4\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  0.96s | test accuracy    0.735 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   2 | time:  0.97s | test accuracy    0.788 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   3 | time:  0.88s | test accuracy    0.823 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   4 | time:  1.13s | test accuracy    0.832 \n",
      "\t-----------------------------------------------------------\n",
      "fold: 5\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  1.03s | test accuracy    0.947 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   2 | time:  0.91s | test accuracy    0.885 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   3 | time:  0.97s | test accuracy    0.885 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   4 | time:  1.07s | test accuracy    0.885 \n",
      "\t-----------------------------------------------------------\n",
      "fold: 6\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  0.92s | test accuracy    0.982 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   2 | time:  0.86s | test accuracy    0.956 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   3 | time:  0.98s | test accuracy    0.938 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   4 | time:  1.02s | test accuracy    0.973 \n",
      "\t-----------------------------------------------------------\n",
      "fold: 7\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  1.07s | test accuracy    0.973 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   2 | time:  0.98s | test accuracy    0.982 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   3 | time:  0.95s | test accuracy    0.982 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   4 | time:  1.09s | test accuracy    0.965 \n",
      "\t-----------------------------------------------------------\n",
      "fold: 8\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  0.99s | test accuracy    1.000 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   2 | time:  0.97s | test accuracy    1.000 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   3 | time:  1.12s | test accuracy    1.000 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   4 | time:  1.07s | test accuracy    1.000 \n",
      "\t-----------------------------------------------------------\n",
      "fold: 9\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  1.03s | test accuracy    1.000 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   2 | time:  1.09s | test accuracy    1.000 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   3 | time:  1.01s | test accuracy    1.000 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   4 | time:  0.83s | test accuracy    1.000 \n",
      "\t-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the IMDB training set\n",
    "crossValidation(train_texts, classes, model, single_layer_model_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight tensor([[-0.4321, -0.2709, -0.2175, -0.3858,  1.2870],\n",
      "        [-0.5192,  0.1369, -0.0348, -0.5189, -0.5206],\n",
      "        [-0.9257,  0.0254, -1.0665, -1.4876,  0.6701],\n",
      "        ...,\n",
      "        [-0.7875, -0.3922, -0.4267,  0.2914,  1.0996],\n",
      "        [ 0.4884, -1.1890,  1.4996, -2.0429, -0.4221],\n",
      "        [ 1.9716, -0.1226,  0.3288,  1.2408,  0.0205]])\n",
      "hidden_layer.weight tensor([[ 3.4457e-02,  3.0551e-01,  4.2761e-01,  2.3980e-01, -9.1255e-01],\n",
      "        [ 2.3182e+00,  1.4155e+00,  7.2949e-01, -1.2646e+00,  1.7920e+00],\n",
      "        [-4.5588e-01, -2.2772e-01,  4.6248e-02,  4.0328e-01, -7.7850e-01],\n",
      "        [-2.2462e-01, -1.5815e-01,  3.1412e-01,  5.2820e-01, -7.6246e-01],\n",
      "        [-2.2733e+00, -1.3973e+00, -9.2936e-01,  1.2767e+00, -1.4810e+00],\n",
      "        [-1.1290e+00, -8.2203e-01,  9.5091e-02,  1.0133e+00, -1.0069e+00],\n",
      "        [-1.2967e+00, -8.4557e-01,  5.6132e-02,  6.7173e-01, -1.6004e+00],\n",
      "        [-4.8835e-01,  1.0687e-01,  1.9625e-01,  2.6353e-01, -1.2168e+00],\n",
      "        [ 5.6281e-02,  2.1573e-01, -2.2598e-01,  3.5773e-01, -4.5217e-01],\n",
      "        [-1.2288e+00, -2.6771e-01,  3.8041e-02,  8.6632e-01, -1.1595e+00],\n",
      "        [ 7.1779e-01,  7.3298e-01,  5.5905e-01, -1.6056e-02,  3.6018e-02],\n",
      "        [-2.4886e+00, -8.4324e-01, -4.3282e-01,  1.3416e+00, -1.6213e+00],\n",
      "        [-6.8732e+00, -4.1382e+00, -1.5967e+00,  4.3479e+00, -5.0182e+00],\n",
      "        [-4.0492e-01,  4.2043e-01,  4.8640e-01,  1.0078e-02, -5.9210e-01],\n",
      "        [-3.4675e-01, -5.7506e-01,  4.8792e-02,  6.1785e-01, -5.7460e-01],\n",
      "        [-6.5564e-01, -6.2971e-01, -4.4887e-01,  9.1808e-01, -1.2780e+00],\n",
      "        [-3.8826e+00, -2.2654e+00, -1.2662e+00,  2.6817e+00, -2.6913e+00],\n",
      "        [-1.4279e+00, -4.0163e-01, -2.3331e-01,  3.4847e-01, -1.0864e+00],\n",
      "        [-5.5048e-01, -3.6575e-01,  1.5397e-01,  8.4911e-01, -1.4510e+00],\n",
      "        [-8.1265e-01, -6.3433e-01,  6.1174e-02,  5.8749e-01, -1.1452e+00],\n",
      "        [ 1.9990e-01,  2.8573e-02, -2.1279e-01,  1.9159e-01, -4.4297e-01],\n",
      "        [-9.2010e+00, -5.2446e+00, -2.3428e+00,  5.7265e+00, -6.5647e+00],\n",
      "        [ 1.6789e+00,  5.8946e-01,  7.1367e-01, -1.1185e+00,  1.2409e+00],\n",
      "        [-2.9647e-02,  1.7346e-01, -1.7347e-01, -9.0609e-03, -3.7171e-01],\n",
      "        [-4.4222e+00, -2.3654e+00, -1.2731e+00,  2.5216e+00, -3.1599e+00],\n",
      "        [-6.4008e-01, -6.5163e-02, -2.2102e-01,  6.7602e-01, -8.5791e-01],\n",
      "        [-1.9865e+00, -5.6010e-01, -7.7390e-01,  1.3455e+00, -1.6473e+00],\n",
      "        [-1.0918e+00, -9.4290e-01, -6.9067e-03,  8.6780e-01, -1.6955e+00],\n",
      "        [-1.1665e+00, -3.3803e-03, -1.7482e-01,  3.6941e-01, -1.4164e+00],\n",
      "        [-1.3130e+01, -7.6031e+00, -3.3518e+00,  8.0529e+00, -9.7177e+00],\n",
      "        [-1.4454e+00, -1.1390e+00, -7.8984e-01,  1.6004e+00, -1.6922e+00],\n",
      "        [-1.5691e+00, -4.4511e-01, -4.0701e-01,  1.1248e+00, -1.1959e+00],\n",
      "        [-2.2201e+00, -1.0050e+00, -3.8752e-01,  1.4344e+00, -1.7740e+00],\n",
      "        [-3.0726e+00, -1.4630e+00, -7.7979e-01,  1.8684e+00, -2.3350e+00],\n",
      "        [-1.1570e+00, -3.1658e-01, -3.8608e-01,  6.3195e-01, -1.3340e+00],\n",
      "        [-2.9056e-01, -5.2775e-02,  2.8988e-02,  6.9593e-01, -9.8658e-01],\n",
      "        [-3.1162e-01,  1.6796e-01,  3.8877e-01,  7.3332e-01, -7.4000e-01],\n",
      "        [-2.4813e-01,  3.6787e-01, -6.6940e-02,  7.7733e-01, -7.8785e-01],\n",
      "        [-2.8680e-01, -2.3479e-01,  6.2314e-02,  5.7265e-02, -8.5101e-01],\n",
      "        [-3.7711e-01,  1.1281e-01,  3.9208e-01, -1.2342e-02, -6.3789e-01]])\n",
      "hidden_layer.bias tensor([-2.7820,  0.7044, -2.9260, -2.8950, -1.4737, -2.4900, -2.2195, -2.7876,\n",
      "        -2.9218, -2.4910, -1.9491, -1.4184,  0.3422, -2.8888, -2.9802, -2.5493,\n",
      "        -0.2897, -2.5848, -2.5619, -2.6736, -2.9228,  0.6415,  0.1672, -2.9543,\n",
      "        -0.1635, -2.8295, -1.6564, -2.1661, -2.5505,  1.1592, -1.6717, -2.2044,\n",
      "        -1.3959, -0.6759, -2.4751, -2.8178, -2.8547, -2.8495, -2.9481, -2.9405])\n",
      "top_layer.weight tensor([[ 2.1571e-01,  3.5855e+00, -2.0697e-01, -3.4350e-02, -1.5278e+00,\n",
      "         -6.8055e-01, -1.0062e+00, -1.6970e-01,  2.8706e-01, -7.3062e-01,\n",
      "          1.0038e+00, -1.4354e+00, -5.5445e+00,  2.5518e-01, -6.9928e-02,\n",
      "         -6.7745e-01, -2.8681e+00, -7.6655e-01, -4.0861e-01, -5.2683e-01,\n",
      "          3.3312e-01, -7.6623e+00,  2.5485e+00,  2.3219e-01, -3.2592e+00,\n",
      "         -2.3005e-01, -1.3088e+00, -9.3355e-01, -6.5493e-01, -1.1805e+01,\n",
      "         -1.3914e+00, -9.7977e-01, -1.5282e+00, -2.0433e+00, -7.6931e-01,\n",
      "         -2.0497e-01, -4.9629e-02, -1.4636e-02, -5.3301e-02,  1.6894e-01],\n",
      "        [-2.4423e-01, -3.6524e+00,  1.0559e-01,  3.0477e-02,  1.5919e+00,\n",
      "          8.3017e-01,  8.0991e-01,  1.5447e-01, -1.9704e-01,  6.4807e-01,\n",
      "         -1.1418e+00,  1.6078e+00,  5.5101e+00, -1.8661e-01,  2.9655e-01,\n",
      "          6.0163e-01,  2.9390e+00,  6.1936e-01,  6.1520e-01,  5.4367e-01,\n",
      "         -2.8192e-01,  7.6478e+00, -2.4066e+00, -3.8881e-01,  3.1521e+00,\n",
      "          3.9914e-01,  1.3463e+00,  9.1719e-01,  5.1226e-01,  1.1793e+01,\n",
      "          1.2599e+00,  9.5302e-01,  1.5003e+00,  2.2703e+00,  6.4663e-01,\n",
      "          1.4851e-01,  6.9488e-03,  5.5653e-02, -1.6643e-02, -1.0823e-01]])\n",
      "top_layer.bias tensor([ 11.3314, -11.3704])\n"
     ]
    }
   ],
   "source": [
    "# Print the parameters for the best performing model's embedding, hidden, and output layers\n",
    "# Note: the embedding layer is truncated\n",
    "best_model = TextClassifierSingleHiddenLayer(vocab_size, embedding_dims, num_hidden_nodes, num_classes)\n",
    "best_model.load_state_dict(torch.load(single_layer_model_out_path))\n",
    "\n",
    "for name, param in best_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Optimization Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset class, batch collator, and model class were created, the parameter optimization process began with selecting hyperparameters. I wasn't really thinking about results at this stage; I ran into memory issues running the model on my graphics card, so I switched to using the CPU and lowered the embedding dimensions to 5 and hidden layer nodes to 40. With these hyperparameters, I was able to start training the model, fixing tensor dimension errors as they popped up. For the criterion, or loss function, I chose binary cross entropy because the classification problem is a binary one. I used stochastic gradient descent as the optimizer, following what was done in the pytorch tutorial. I may have achieved better results more quickly using Adam, but this is a simple problem so I didn't worry about it too much.\n",
    "\n",
    "The training loop involved the followings steps:<br>\n",
    "    1) Create train/test dataloaders from the random 10-fold split.<br>\n",
    "    2) Start loop for each epoch. For each epoch, send the train dataloader to the train function and iterate through the dataloader by batch size.<br>\n",
    "    3) Clear the gradients for all optimizer parameters.<br>\n",
    "    4) Predict the batch labels.<br>\n",
    "    5) Compute the loss.<br>\n",
    "    6) Compute the gradient for every parameter.<br>\n",
    "    7) Clip the gradients to avoid gradient value explosion.<br>\n",
    "    8) Update the parameters with the current gradient.<br>\n",
    "    9) Compute the running total accuracy and count values for the epoch.<br>\n",
    "    10) Compute an epoch accuracy score by evaluating the model on the test dataloader.<br>\n",
    "    11) Update the learning rate if the current total accuracy is greater than the epoch accuracy (the epoch failed to improve model performance). otherwise update the total accuracy value to the epoch accuracy and save the models parameters.<br>\n",
    "    12) Repeat steps 1-11 for the number of cross fold validation splits (10).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Test Feed-Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the entire training dataset\n",
    "batch_size = 5\n",
    "criterion = nn.BCELoss()\n",
    "training_dataset = ImdbDataset(train_texts, classes)\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "training_set_accuracy1 = evaluate(training_dataloader, best_model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model's accuracy on the entire training dataset: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Report the model's accuracy on the entire training dataset\n",
    "print(\"Best model's accuracy on the entire training dataset: {:.3}\".format(training_set_accuracy1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Feed-Forward Neural Network with More Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifierTwoHiddenLayers(nn.Module):\n",
    "    \"\"\"\n",
    "    Text classifier model.\n",
    "    Consists of a word embedding layer, two hidden layers, top layer, and activation function.\n",
    "    \n",
    "    Sources: \n",
    "        https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "        https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/\n",
    "        http://www.cse.chalmers.se/~richajo/nlp2019/l2/Text%20classification%20using%20a%20CBoW%20representation.html\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dims, n_layer1_nodes, n_layer2_nodes, num_classes):\n",
    "        super(TextClassifierTwoHiddenLayers, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dims, sparse=True)\n",
    "        self.hidden_layer1 = nn.Linear(embedding_dims, n_layer1_nodes)\n",
    "        self.hidden_layer2 = nn.Linear(n_layer1_nodes, n_layer2_nodes)\n",
    "        self.top_layer = nn.Linear(n_layer2_nodes, num_classes)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initranges = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.hidden_layer1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.hidden_layer2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.hidden_layer.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        hidden1 = self.act(self.hidden_layer1(embedded))\n",
    "        self.dropout(hidden1)\n",
    "        hidden2 = self.act(self.hidden_layer2(hidden1))\n",
    "        scores = self.top_layer(hidden2)\n",
    "        return self.act(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the double hidden layer text classifier model\n",
    "vocab_size = len(train_vocabulary)\n",
    "embedding_dims = 5\n",
    "n_layer1_nodes = 5\n",
    "n_layer2_nodes = 10\n",
    "num_classes = 2\n",
    "\n",
    "two_hidden_layer_model = TextClassifierTwoHiddenLayers(vocab_size, embedding_dims, n_layer1_nodes, n_layer2_nodes, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model output path\n",
    "double_layer_model_out_path = os.path.join(os.getcwd(), 'part_2_output', 'best_double_hidden_layer_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold: 0\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  1.09s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   2 | time:  0.90s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   3 | time:  0.96s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   4 | time:  1.05s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   5 | time:  0.96s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   6 | time:  0.97s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   7 | time:  1.01s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   8 | time:  0.96s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   9 | time:  1.13s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  10 | time:  1.02s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  11 | time:  1.22s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  12 | time:  1.19s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  13 | time:  1.05s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  14 | time:  1.00s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  15 | time:  0.93s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  16 | time:  0.96s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  17 | time:  0.98s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  18 | time:  1.03s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  19 | time:  0.94s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  20 | time:  0.93s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  21 | time:  0.98s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  22 | time:  1.07s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  23 | time:  0.96s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  24 | time:  1.00s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch  25 | time:  0.94s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n",
      "fold: 1\n",
      "\t-----------------------------------------------------------\n",
      "\t| end of epoch   1 | time:  0.98s | test accuracy    0.500 \n",
      "\t-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the double hidden layer model with the IMDB training set\n",
    "crossValidation(train_texts, classes, two_hidden_layer_model, double_layer_model_out_path, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parameters for the best performing model's embedding, hidden, and output layers\n",
    "# Note: the embedding layer is truncated\n",
    "best_double_layer_model = TextClassifierTwoHiddenLayers(vocab_size, embedding_dims, n_layer1_nodes, n_layer2_nodes, num_classes)\n",
    "best_double_layer_model.load_state_dict(torch.load(double_layer_model_out_path))\n",
    "\n",
    "for name, param in best_double_layer_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the entire training dataset\n",
    "batch_size = 5\n",
    "criterion = nn.BCELoss()\n",
    "training_dataset = ImdbDataset(train_texts, classes)\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "training_set_accuracy2 = evaluate(training_dataloader, best_double_layer_model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the model's accuracy on the entire training dataset\n",
    "print(\"Best model's accuracy on the entire training dataset: {:.3}\".format(training_set_accuracy2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Neural Network Classification on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data\n",
    "test_files = glob.glob(os.path.join(os.getcwd(), 'imdb_test_data', '*'))\n",
    "test_texts = [Path(file).read_text() for file in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the test texts\n",
    "for index,text in enumerate(test_texts):\n",
    "    test_texts[index] = preprocessText(text, cached_stopwords, wordnet_lemmatizer)\n",
    "    \n",
    "for index,text in enumerate(test_texts):\n",
    "    test_texts[index] = replaceUnk(text, train_vocabulary, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of filenames and test_text indices\n",
    "test_file_names_texts = {}\n",
    "\n",
    "for index,text in enumerate(test_texts):\n",
    "    file_name = test_files[index].rsplit('\\\\', 1)[1].rsplit('.txt')[0]\n",
    "    test_file_names_texts[index] = file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_texts):\n",
    "    \"\"\"\n",
    "    Run the specified model in the specified test texts and return the predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_accuracy, total_count = 0, 0\n",
    "    batch_size = 5\n",
    "    criterion = nn.BCELoss()\n",
    "    test_classes = [None] * len(test_texts)\n",
    "    test_dataset = ImdbDataset(test_texts, test_classes)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "    predictions = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(test_dataloader):\n",
    "            predicted_label = model(text, offsets)       \n",
    "            labels = predicted_label.argmax(1).tolist()\n",
    "            \n",
    "            for index in range(0, len(labels)):\n",
    "                text_index = (idx * batch_size) + index\n",
    "                predictions[text_index] = labels[index]\n",
    "                \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the entire training dataset\n",
    "if training_set_accuracy1 > training_set_accuracy2:\n",
    "    test_predictions = predict(best_model, test_texts)\n",
    "else:\n",
    "    test_predictions = predict(best_double_layer_model, test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputPredictions(output_file_path, target_label, predictions, test_file_dict):\n",
    "    \"\"\"\n",
    "    Write the names of the files classified by the model as belonging to the target\n",
    "    label class to the specified output file.\n",
    "    \"\"\"\n",
    "    with open(output_file_path, 'w') as file:\n",
    "        for test_file_index in predictions.keys():\n",
    "            if predictions[test_file_index] == target_label:\n",
    "                test_file_name = test_file_dict[test_file_index]\n",
    "                line = test_file_name + '\\n'\n",
    "                file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group file names according to their predicted label and write file names to output files\n",
    "pos_predictions_out = os.path.join(os.getcwd(), 'part_2_output', 'pos.txt')\n",
    "neg_predictions_out = os.path.join(os.getcwd(), 'part_2_output', 'neg.txt')\n",
    "outputPredictions(pos_predictions_out, 1, test_predictions, test_file_names_texts)\n",
    "outputPredictions(neg_predictions_out, 0, test_predictions, test_file_names_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
